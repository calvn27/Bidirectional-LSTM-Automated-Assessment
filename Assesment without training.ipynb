{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import yake\n",
    "from langdetect import detect\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r'C:\\Users\\ncalvaresi\\Documents\\my_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\ncalvaresi\\Documents\\Safety_data_full.txt\", delimiter='\\t', encoding='latin1', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncalvaresi\\AppData\\Local\\Temp\\ipykernel_36228\\3819309048.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['FollowupAction'].fillna(\"None\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={df.columns[0]: 'State', df.columns[6]: 'Observation', df.columns[7]: 'FollowupAction'}, inplace=True)\n",
    "\n",
    "# Replace and drop NAs\n",
    "df['FollowupAction'].fillna(\"None\", inplace=True)\n",
    "df.dropna(subset=['Observation'], inplace=True)\n",
    "\n",
    "df = df[['Observation', 'Severity', 'Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "0    63773\n",
      "1    43282\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_entries = [\n",
    "    {'Observation': 'Spill', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Leak', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Fire', 'Severity': 'UNSAFE', 'Category': 'Fire Prevention'},\n",
    "    {'Observation': 'Puddle of water', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Gap in the railing', 'Severity': 'UNSAFE', 'Category': 'Fall Protection'},\n",
    "    {'Observation': 'He was wearing his hard hat incorrectly', 'Severity': 'UNSAFE', 'Category': 'PPE'}\n",
    "     \n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "new_entries_df = pd.DataFrame(new_entries)\n",
    "\n",
    "# Add the new entries to the original DataFrame using pd.concat\n",
    "df = pd.concat([df, new_entries_df], ignore_index=True)\n",
    "\n",
    "# Filter for 'UNSAFE' and 'SAFE'\n",
    "filtered_df = df[df['Severity'].isin(['UNSAFE', 'SAFE'])]\n",
    "\n",
    "# Replace string 'nan' with actual NaN\n",
    "#filtered_df['Category'].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "# Drop rows where the Category is NaN\n",
    "filtered_df.dropna(subset=['Category'], inplace=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['sentiment'] = label_encoder.fit_transform(filtered_df['Severity'])\n",
    "\n",
    "# Check data balance\n",
    "print(filtered_df['sentiment'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    # Filter out None or common placeholder strings\n",
    "    if text is None or str(text).lower() in ['none', 'n/a', 'na', 'null']:\n",
    "        return False\n",
    "    # Filter out single letters, random numbers, and gibberish\n",
    "    if len(text) == 1 and text.isalpha():  # Filter out single letters\n",
    "        return False\n",
    "    if re.match(r'^[\\W\\d]*$', text):  # Filter out strings with only non-word characters or digits\n",
    "        return False\n",
    "    # Filter out non-English text\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang != 'en':  # Change 'en' to 'de' or 'es' for German or Spanish, if needed\n",
    "            return False\n",
    "    except:\n",
    "        return False  # Handle cases where language detection fails\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_df = filtered_df[filtered_df['Observation'].apply(filter_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_filtered_df[['Observation', 'sentiment', 'Category']]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['Observation']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Encode categories and severity\n",
    "label_encoder_category = LabelEncoder()\n",
    "df['category'] = label_encoder_category.fit_transform(df['Category'])\n",
    "\n",
    "label_encoder_severity = LabelEncoder()\n",
    "df['severity'] = label_encoder_severity.fit_transform(df['Severity'])\n",
    "\n",
    "# Prepare dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "train_df['encoded'] = train_df['Observation'].apply(encode)\n",
    "test_df['encoded'] = test_df['Observation'].apply(encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    else:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "train_df['padded'] = train_df['encoded'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "test_df['padded'] = test_df['encoded'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "train_categories = torch.tensor(train_df['category'].tolist())\n",
    "test_categories = torch.tensor(test_df['category'].tolist())\n",
    "\n",
    "train_severities = torch.tensor(train_df['severity'].tolist())\n",
    "test_severities = torch.tensor(test_df['severity'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, categories, severities):\n",
    "        self.texts = texts\n",
    "        self.categories = categories\n",
    "        self.severities = severities\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        category = self.categories[idx]\n",
    "        severity = self.severities[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(category, dtype=torch.long), torch.tensor(severity, dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(train_df['padded'].tolist(), train_categories, train_severities)\n",
    "test_dataset = TextDataset(test_df['padded'].tolist(), test_categories, test_severities)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=15, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.context_vector = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "        nn.init.xavier_uniform_(self.attention_weights)\n",
    "        nn.init.xavier_uniform_(self.context_vector)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        scores = torch.tanh(torch.matmul(hidden_states, self.attention_weights))\n",
    "        scores = torch.matmul(scores, self.context_vector).squeeze(-1)\n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=1)\n",
    "        weighted_sum = torch.sum(hidden_states * attention_weights.unsqueeze(-1), dim=1)\n",
    "        return weighted_sum\n",
    "\n",
    "class EnhancedTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes_category, num_classes_severity):\n",
    "        super(EnhancedTextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, 128, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.attention = Attention(128 * 2)  # Bidirectional LSTM has hidden_size*2\n",
    "        self.fc_category = nn.Linear(128 * 2, num_classes_category)\n",
    "        self.fc_severity = nn.Linear(128 * 2, num_classes_severity)\n",
    "        self.dropout = nn.Dropout(0.5)  # Added dropout for regularization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        output_category = self.fc_category(x)\n",
    "        output_severity = self.fc_severity(x)\n",
    "        return output_category, output_severity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_observation(observation, model, tokenizer, vocab, max_length, device, label_encoder_category, label_encoder_severity):\n",
    "    tokens = tokenizer(observation)\n",
    "    encoded = [vocab[token] for token in tokens]\n",
    "    padded = pad_sequence(encoded, max_length)\n",
    "    input_tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_category, output_severity = model(input_tensor)\n",
    "        \n",
    "        # Calculate softmax probabilities for both category and severity\n",
    "        probabilities_category = F.softmax(output_category, dim=1)\n",
    "        probabilities_severity = F.softmax(output_severity, dim=1)\n",
    "        \n",
    "        # Get the predicted labels and confidence scores for both category and severity\n",
    "        confidence_category, predicted_label_category = torch.max(probabilities_category, dim=1)\n",
    "        confidence_severity, predicted_label_severity = torch.max(probabilities_severity, dim=1)\n",
    "    \n",
    "    # Convert the labels to the original category names and severity labels\n",
    "    predicted_category = label_encoder_category.inverse_transform([predicted_label_category.item()])[0]\n",
    "    predicted_severity = label_encoder_severity.inverse_transform([predicted_label_severity.item()])[0]\n",
    "    \n",
    "    # Get the confidence scores\n",
    "    confidence_score_category = confidence_category.item()\n",
    "    confidence_score_severity = confidence_severity.item()\n",
    "    \n",
    "    return {\n",
    "        'predicted_category': predicted_category,\n",
    "        'confidence_score_category': confidence_score_category,\n",
    "        'predicted_severity': predicted_severity,\n",
    "        'confidence_score_severity': confidence_score_severity\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_percentages(observation, dataset, predicted_label, other_label):\n",
    "    \n",
    "    lemmatized_observation = clean_text(observation)\n",
    "    \n",
    "    # Lemmatize the dataset observations\n",
    "    #dataset['Lemmatized_Observation'] = dataset['Observation'].apply(lemmatize_text)\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_threshold = 0.8\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(lemmatized_observation)\n",
    "    \n",
    "    keywords_perc = {}\n",
    "    other_keywords_perc = {}\n",
    "\n",
    "    for keyword, _ in keywords:\n",
    "        # Filter dataset to include only observations containing the keyword\n",
    "        keyword_obs_df = dataset[dataset['Observation'].str.contains(keyword, case=False, na=False)]\n",
    "        \n",
    "        # Total number of observations containing the keyword\n",
    "        total_keyword_obs = keyword_obs_df.shape[0]\n",
    "        \n",
    "        # Number of observations containing the keyword and having the predicted label\n",
    "        label_keyword_obs = keyword_obs_df[keyword_obs_df['sentiment'] == predicted_label].shape[0]\n",
    "        other_label_keyword_obs = keyword_obs_df[keyword_obs_df['sentiment'] == other_label].shape[0]\n",
    "        \n",
    "        if total_keyword_obs > 0:\n",
    "            perc = label_keyword_obs / total_keyword_obs\n",
    "            perc_other = other_label_keyword_obs / total_keyword_obs\n",
    "        else:\n",
    "            perc = 0\n",
    "            perc_other = 0\n",
    "        \n",
    "        keywords_perc[keyword] = (perc, perc_other)\n",
    "    \n",
    "    return keywords_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedTextClassificationModel(\n",
       "  (embedding): Embedding(25999, 100)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (attention): Attention()\n",
       "  (fc_category): Linear(in_features=256, out_features=31, bias=True)\n",
       "  (fc_severity): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_SIZE = 100\n",
    "NUM_CLASSES_CATEGORY = len(label_encoder_category.classes_)\n",
    "NUM_CLASSES_SEVERITY = len(label_encoder_severity.classes_)\n",
    "# Recreate the model architecture\n",
    "loaded_model = EnhancedTextClassificationModel(VOCAB_SIZE, EMBED_SIZE, NUM_CLASSES_CATEGORY, NUM_CLASSES_SEVERITY)\n",
    "\n",
    "# Load the saved weights into the model\n",
    "loaded_model.load_state_dict(torch.load(r'C:\\Users\\ncalvaresi\\Documents\\my_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Materials/Tools, Confidence Score: 54.2326%\n",
      "Predicted Severity: SAFE, Confidence Score: 89.0431%\n",
      "\n",
      "Keyword: small leak, Percentage of small leak in SAFE: 42.8571% and Percentage of small leak in the other severity: 57.1429%\n",
      "Keyword: storage room, Percentage of storage room in SAFE: 25.0000% and Percentage of storage room in the other severity: 75.0000%\n",
      "Keyword: small, Percentage of small in SAFE: 52.1739% and Percentage of small in the other severity: 47.8261%\n",
      "Keyword: leak, Percentage of leak in SAFE: 36.2617% and Percentage of leak in the other severity: 63.7383%\n",
      "Keyword: storage, Percentage of storage in SAFE: 45.1282% and Percentage of storage in the other severity: 54.8718%\n",
      "Keyword: room, Percentage of room in SAFE: 48.9590% and Percentage of room in the other severity: 51.0410%\n",
      "Keyword: damage, Percentage of damage in SAFE: 41.9291% and Percentage of damage in the other severity: 58.0709%\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "example_text = \"There was a small leak in the storage room that could damage everything.\"\n",
    "prediction = predict_observation(example_text, loaded_model, tokenizer, vocab, MAX_SEQUENCE_LENGTH, device, label_encoder_category, label_encoder_severity)\n",
    "\n",
    "predicted_category = prediction['predicted_category']\n",
    "predicted_severity = prediction['predicted_severity']\n",
    "confidence_score_category = prediction['confidence_score_category']\n",
    "confidence_score_severity = prediction['confidence_score_severity']\n",
    "\n",
    "# Assuming 0 and 1 are the only labels for severity (binary classification)\n",
    "predicted_label_severity = label_encoder_severity.transform([predicted_severity])[0]\n",
    "other_label_severity = 1 - predicted_label_severity\n",
    "\n",
    "keywords_percentages = extract_keywords_percentages(example_text, new_filtered_df, predicted_label_severity, other_label_severity)\n",
    "\n",
    "print(f\"Predicted Category: {predicted_category}, Confidence Score: {confidence_score_category:.4%}\")\n",
    "print(f\"Predicted Severity: {predicted_severity}, Confidence Score: {confidence_score_severity:.4%}\")\n",
    "print(\"\")\n",
    "for keyword, (perc, perc_other) in keywords_percentages.items():\n",
    "    print(f\"Keyword: {keyword}, Percentage of {keyword} in {predicted_severity}: {perc:.4%} and Percentage of {keyword} in the other severity: {perc_other:.4%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
