{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\ncalvaresi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "import re\n",
    "import spacy\n",
    "import yake\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncalvaresi\\AppData\\Local\\Temp\\ipykernel_2368\\3447139848.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['FollowupAction'].fillna(\"None\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\ncalvaresi\\Documents\\Safety_data_full.txt\", delimiter='\\t', encoding='latin1', on_bad_lines='skip')\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={df.columns[0]: 'State', df.columns[6]: 'Observation', df.columns[7]: 'FollowupAction'}, inplace=True)\n",
    "\n",
    "# Replace and drop NAs\n",
    "df['FollowupAction'].fillna(\"None\", inplace=True)\n",
    "df.dropna(subset=['Observation'], inplace=True)\n",
    "\n",
    "df = df[['Observation', 'Severity', 'Category']]\n",
    "\n",
    "new_entries = [\n",
    "    {'Observation': 'Spill', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Leak', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Fire', 'Severity': 'UNSAFE', 'Category': 'Fire Prevention'},\n",
    "    {'Observation': 'Puddle of water', 'Severity': 'UNSAFE', 'Category': 'Slips/Trip Hazards'},\n",
    "    {'Observation': 'Gap in the railing', 'Severity': 'UNSAFE', 'Category': 'Fall Protection'},\n",
    "    {'Observation': 'He was wearing his hard hat incorrectly', 'Severity': 'UNSAFE', 'Category': 'PPE'}\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "new_entries_df = pd.DataFrame(new_entries)\n",
    "\n",
    "# Add the new entries to the original DataFrame using pd.concat\n",
    "df = pd.concat([df, new_entries_df], ignore_index=True)\n",
    "\n",
    "# Filter for 'UNSAFE' and 'SAFE'\n",
    "filtered_df = df[df['Severity'].isin(['UNSAFE', 'SAFE'])]\n",
    "\n",
    "# Drop rows where the Category is NaN\n",
    "filtered_df.dropna(subset=['Category'], inplace=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['sentiment'] = label_encoder.fit_transform(filtered_df['Severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    if text is None or str(text).lower() in ['none', 'n/a', 'na', 'null']:\n",
    "        return False\n",
    "    if len(text) == 1 and text.isalpha(): \n",
    "        return False\n",
    "    if re.match(r'^[\\W\\d]*$', text): \n",
    "        return False\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang != 'en':  \n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_df = filtered_df[filtered_df['Observation'].apply(filter_text)]\n",
    "\n",
    "new_df = new_filtered_df[['Observation', 'sentiment', 'Category']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['Observation']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "label_encoder_category = LabelEncoder()\n",
    "df['category'] = label_encoder_category.fit_transform(df['Category'])\n",
    "\n",
    "label_encoder_severity = LabelEncoder()\n",
    "df['severity'] = label_encoder_severity.fit_transform(df['Severity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "train_df['encoded'] = train_df['Observation'].apply(encode)\n",
    "test_df['encoded'] = test_df['Observation'].apply(encode)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    else:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "train_df['padded'] = train_df['encoded'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "test_df['padded'] = test_df['encoded'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "train_categories = torch.tensor(train_df['category'].tolist())\n",
    "test_categories = torch.tensor(test_df['category'].tolist())\n",
    "\n",
    "train_severities = torch.tensor(train_df['severity'].tolist())\n",
    "test_severities = torch.tensor(test_df['severity'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, categories, severities):\n",
    "        self.texts = texts\n",
    "        self.categories = categories\n",
    "        self.severities = severities\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        category = self.categories[idx]\n",
    "        severity = self.severities[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(category, dtype=torch.long), torch.tensor(severity, dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(train_df['padded'].tolist(), train_categories, train_severities)\n",
    "test_dataset = TextDataset(test_df['padded'].tolist(), test_categories, test_severities)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=15, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.context_vector = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "        nn.init.xavier_uniform_(self.attention_weights)\n",
    "        nn.init.xavier_uniform_(self.context_vector)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        scores = torch.tanh(torch.matmul(hidden_states, self.attention_weights))\n",
    "        scores = torch.matmul(scores, self.context_vector).squeeze(-1)\n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=1)\n",
    "        weighted_sum = torch.sum(hidden_states * attention_weights.unsqueeze(-1), dim=1)\n",
    "        return weighted_sum\n",
    "\n",
    "class EnhancedTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes_category, num_classes_severity):\n",
    "        super(EnhancedTextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, 128, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.attention = Attention(128 * 2)  \n",
    "        self.fc_category = nn.Linear(128 * 2, num_classes_category)\n",
    "        self.fc_severity = nn.Linear(128 * 2, num_classes_severity)\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        output_category = self.fc_category(x)\n",
    "        output_severity = self.fc_severity(x)\n",
    "        return output_category, output_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_percentages(observation, dataset, predicted_label, other_label):\n",
    "    lemmatized_observation = clean_text(observation)\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_threshold = 0.8\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(lemmatized_observation)\n",
    "    \n",
    "    keywords_perc = {}\n",
    "\n",
    "    for keyword, _ in keywords:\n",
    "        keyword_obs_df = dataset[dataset['Observation'].str.contains(keyword, case=False, na=False)]\n",
    "        total_keyword_obs = keyword_obs_df.shape[0]\n",
    "        \n",
    "        label_keyword_obs = keyword_obs_df[keyword_obs_df['sentiment'] == predicted_label].shape[0]\n",
    "        other_label_keyword_obs = keyword_obs_df[keyword_obs_df['sentiment'] == other_label].shape[0]\n",
    "        \n",
    "        if total_keyword_obs > 0:\n",
    "            perc = label_keyword_obs / total_keyword_obs\n",
    "            perc_other = other_label_keyword_obs / total_keyword_obs\n",
    "        else:\n",
    "            perc = 0\n",
    "            perc_other = 0\n",
    "        \n",
    "        keywords_perc[keyword] = (perc, perc_other)\n",
    "    \n",
    "    return keywords_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_analyze_observation(observation, model, tokenizer, vocab, max_length, device, label_encoder_category, label_encoder_severity, dataset):\n",
    "    def pad_sequence(seq, max_len):\n",
    "        if len(seq) > max_len:\n",
    "            return seq[:max_len]\n",
    "        else:\n",
    "            return seq + [0] * (max_len - len(seq))\n",
    "    \n",
    "    tokens = tokenizer(observation)\n",
    "    encoded = [vocab[token] for token in tokens]\n",
    "    padded = pad_sequence(encoded, max_length)\n",
    "    input_tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_category, output_severity = model(input_tensor)\n",
    "        probabilities_category = F.softmax(output_category, dim=1)\n",
    "        probabilities_severity = F.softmax(output_severity, dim=1)\n",
    "        confidence_category, predicted_label_category = torch.max(probabilities_category, dim=1)\n",
    "        confidence_severity, predicted_label_severity = torch.max(probabilities_severity, dim=1)\n",
    "    \n",
    "    predicted_category = label_encoder_category.inverse_transform([predicted_label_category.item()])[0]\n",
    "    predicted_severity = label_encoder_severity.inverse_transform([predicted_label_severity.item()])[0]\n",
    "    \n",
    "    confidence_score_category = confidence_category.item()\n",
    "    confidence_score_severity = confidence_severity.item()\n",
    "    \n",
    "    predicted_label_severity = label_encoder_severity.transform([predicted_severity])[0]\n",
    "    other_label_severity = 1 - predicted_label_severity\n",
    "    \n",
    "    keywords_percentages = extract_keywords_percentages(observation, dataset, predicted_label_severity, other_label_severity)\n",
    "    \n",
    "    print(f\"Predicted Category: {predicted_category}, Confidence Score: {confidence_score_category:.4%}\")\n",
    "    print(f\"Predicted Severity: {predicted_severity}, Confidence Score: {confidence_score_severity:.4%}\")\n",
    "    print(\"\")\n",
    "    for keyword, (perc, perc_other) in keywords_percentages.items():\n",
    "        print(f\"Keyword: {keyword}, Percentage of {keyword} in {predicted_severity}: {perc:.4%} and Percentage of {keyword} in the other severity: {perc_other:.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r'C:\\Users\\ncalvaresi\\Documents\\my_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Materials/Tools, Confidence Score: 54.2326%\n",
      "Predicted Severity: SAFE, Confidence Score: 89.0431%\n",
      "\n",
      "Keyword: small leak, Percentage of small leak in SAFE: 42.8571% and Percentage of small leak in the other severity: 57.1429%\n",
      "Keyword: storage room, Percentage of storage room in SAFE: 25.0000% and Percentage of storage room in the other severity: 75.0000%\n",
      "Keyword: small, Percentage of small in SAFE: 52.0710% and Percentage of small in the other severity: 47.9290%\n",
      "Keyword: leak, Percentage of leak in SAFE: 36.2949% and Percentage of leak in the other severity: 63.7051%\n",
      "Keyword: storage, Percentage of storage in SAFE: 44.8454% and Percentage of storage in the other severity: 55.1546%\n",
      "Keyword: room, Percentage of room in SAFE: 49.0476% and Percentage of room in the other severity: 50.9524%\n",
      "Keyword: damage, Percentage of damage in SAFE: 42.2925% and Percentage of damage in the other severity: 57.7075%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_SIZE = 100\n",
    "NUM_CLASSES_CATEGORY = len(label_encoder_category.classes_)\n",
    "NUM_CLASSES_SEVERITY = len(label_encoder_severity.classes_)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Recreate the model architecture\n",
    "loaded_model = EnhancedTextClassificationModel(VOCAB_SIZE, EMBED_SIZE, NUM_CLASSES_CATEGORY, NUM_CLASSES_SEVERITY)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(model))\n",
    "\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Initialize spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example usage\n",
    "example_text = \"There was a small leak in the storage room that could damage everything.\"\n",
    "predict_and_analyze_observation(example_text, loaded_model, tokenizer, vocab, MAX_SEQUENCE_LENGTH, device, label_encoder_category, label_encoder_severity, new_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.36.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ncalvaresi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ncalvaresi\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
